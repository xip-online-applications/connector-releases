{
  "version": 3,
  "sources": ["../../../../../../../libs/kafka-base-service/src/lib/rdkafka-bulk-listener.service.ts"],
  "sourcesContent": ["import { KafkaJS } from '@confluentinc/kafka-javascript';\nimport { Logger } from '@transai/logger';\nimport {\n  BaseConnectorConfig,\n  XodBaseMessageType,\n} from '@xip-online-data/types';\n\nimport { AbstractRdKafkaService } from './abstract-rdkafka-service';\n\nexport class RdKafkaBulkListenerService extends AbstractRdKafkaService {\n  public constructor(\n    baseYamlConfig: BaseConnectorConfig,\n    private readonly callbackFunction: (\n      messages: Array<XodBaseMessageType>,\n    ) => Promise<void>,\n    private readonly bulkApplicable: (\n      messages: Array<XodBaseMessageType>,\n    ) => boolean = () => true,\n  ) {\n    super(baseYamlConfig);\n  }\n\n  public init = async (): Promise<void> => {\n    await this.consumer.connect().catch((e) => {\n      this.logger.error(`Failed to connect to Kafka consumer: ${e.message}`);\n      throw new Error(`Failed to connect to Kafka consumer: ${e.message}`);\n    });\n\n    const topics = [\n      ...(await this.getRegexTopics()),\n      ...this.getRegularTopics(),\n    ];\n\n    await Promise.all([\n      ...topics.map(async (t) => {\n        await this.consumer.subscribe({ topic: t });\n      }),\n    ]);\n\n    if (this.baseYamlConfig.kafka.autoCommitThreshold) {\n      this.logger.error(\n        'autoCommitThreshold is not supported in bulk listener. Please use autoCommitInterval instead',\n      );\n    }\n\n    await this.consumer.run({\n      partitionsConsumedConcurrently:\n        this.baseYamlConfig.kafka.partitionsConsumedConcurrently ?? 1,\n      eachBatchAutoResolve: true,\n      eachBatch: this.#consumeBatch,\n    });\n  };\n\n  #consumeBatch = async ({\n    batch,\n    resolveOffset,\n  }: KafkaJS.EachBatchPayload): Promise<void> => {\n    const messages = batch.messages.map((message) =>\n      JSON.parse(message.value?.toString() ?? ''),\n    );\n    this.logger.debug(\n      'Received batch of messages',\n      messages.length,\n      batch.topic,\n    );\n\n    if (this.bulkApplicable(messages)) {\n      try {\n        await this.callbackFunction(messages).catch((error) => {\n          this.logger.error(\n            `Error in callback function for bulk processing, ${JSON.stringify(error)}`,\n          );\n          throw error;\n        });\n        const { offset } = batch.messages[batch.messages.length - 1];\n        resolveOffset(offset);\n      } catch (error) {\n        this.logger.error(\n          `Error in callback function. Continue as single batch ${JSON.stringify(error)}`,\n        );\n        this.logger.error(\n          `${messages.map((m) => m.value?.toString()).join(',\\n')}`,\n        );\n        await this.#consumeBatchAsSingle(batch.messages, resolveOffset);\n      }\n    } else {\n      this.logger.debug(\n        'Batch processing not applicable. Continue as single batch',\n      );\n      await this.#consumeBatchAsSingle(batch.messages, resolveOffset);\n    }\n  };\n\n  #consumeBatchAsSingle = async (\n    messages: Array<KafkaJS.KafkaMessage>,\n    resolveOffset: (offset: string) => void,\n  ): Promise<void> => {\n    // eslint-disable-next-line no-restricted-syntax\n    for (const message of messages) {\n      try {\n        const parsedMessage = JSON.parse(message.value?.toString() ?? '{}');\n        // eslint-disable-next-line no-await-in-loop\n        await this.callbackFunction([parsedMessage]).catch((error) => {\n          this.logger.error(\n            `Error in callback function for single message processing, ${JSON.stringify(\n              error,\n            )}`,\n          );\n          throw error;\n        });\n        resolveOffset(message.offset);\n      } catch (error) {\n        this.logger.error('Error in callback function', error);\n        this.logger.error(`${message.value?.toString()}`);\n        resolveOffset(message.offset);\n      }\n    }\n  };\n}\n"],
  "mappings": ";;;;;;;;;;;;;;;;;AAAA;AAAA;AAAA;AAAA;AAAA;AAOA,sCAAuC;AAEhC,MAAM,mCAAmC,uDAAuB;AAAA,EAC9D,YACL,gBACiB,kBAGA,iBAEF,MAAM,MACrB;AACA,UAAM,cAAc;AAPH;AAGA;AAOnB,SAAO,OAAO,YAA2B;AACvC,YAAM,KAAK,SAAS,QAAQ,EAAE,MAAM,CAAC,MAAM;AACzC,aAAK,OAAO,MAAM,wCAAwC,EAAE,OAAO,EAAE;AACrE,cAAM,IAAI,MAAM,wCAAwC,EAAE,OAAO,EAAE;AAAA,MACrE,CAAC;AAED,YAAM,SAAS;AAAA,QACb,GAAI,MAAM,KAAK,eAAe;AAAA,QAC9B,GAAG,KAAK,iBAAiB;AAAA,MAC3B;AAEA,YAAM,QAAQ,IAAI;AAAA,QAChB,GAAG,OAAO,IAAI,OAAO,MAAM;AACzB,gBAAM,KAAK,SAAS,UAAU,EAAE,OAAO,EAAE,CAAC;AAAA,QAC5C,CAAC;AAAA,MACH,CAAC;AAED,UAAI,KAAK,eAAe,MAAM,qBAAqB;AACjD,aAAK,OAAO;AAAA,UACV;AAAA,QACF;AAAA,MACF;AAEA,YAAM,KAAK,SAAS,IAAI;AAAA,QACtB,gCACE,KAAK,eAAe,MAAM,kCAAkC;AAAA,QAC9D,sBAAsB;AAAA,QACtB,WAAW,KAAK;AAAA,MAClB,CAAC;AAAA,IACH;AAEA,yBAAgB,OAAO;AAAA,MACrB;AAAA,MACA;AAAA,IACF,MAA+C;AAC7C,YAAM,WAAW,MAAM,SAAS;AAAA,QAAI,CAAC,YACnC,KAAK,MAAM,QAAQ,OAAO,SAAS,KAAK,EAAE;AAAA,MAC5C;AACA,WAAK,OAAO;AAAA,QACV;AAAA,QACA,SAAS;AAAA,QACT,MAAM;AAAA,MACR;AAEA,UAAI,KAAK,eAAe,QAAQ,GAAG;AACjC,YAAI;AACF,gBAAM,KAAK,iBAAiB,QAAQ,EAAE,MAAM,CAAC,UAAU;AACrD,iBAAK,OAAO;AAAA,cACV,mDAAmD,KAAK,UAAU,KAAK,CAAC;AAAA,YAC1E;AACA,kBAAM;AAAA,UACR,CAAC;AACD,gBAAM,EAAE,OAAO,IAAI,MAAM,SAAS,MAAM,SAAS,SAAS,CAAC;AAC3D,wBAAc,MAAM;AAAA,QACtB,SAAS,OAAO;AACd,eAAK,OAAO;AAAA,YACV,wDAAwD,KAAK,UAAU,KAAK,CAAC;AAAA,UAC/E;AACA,eAAK,OAAO;AAAA,YACV,GAAG,SAAS,IAAI,CAAC,MAAM,EAAE,OAAO,SAAS,CAAC,EAAE,KAAK,KAAK,CAAC;AAAA,UACzD;AACA,gBAAM,KAAK,sBAAsB,MAAM,UAAU,aAAa;AAAA,QAChE;AAAA,MACF,OAAO;AACL,aAAK,OAAO;AAAA,UACV;AAAA,QACF;AACA,cAAM,KAAK,sBAAsB,MAAM,UAAU,aAAa;AAAA,MAChE;AAAA,IACF;AAEA,iCAAwB,OACtB,UACA,kBACkB;AAElB,iBAAW,WAAW,UAAU;AAC9B,YAAI;AACF,gBAAM,gBAAgB,KAAK,MAAM,QAAQ,OAAO,SAAS,KAAK,IAAI;AAElE,gBAAM,KAAK,iBAAiB,CAAC,aAAa,CAAC,EAAE,MAAM,CAAC,UAAU;AAC5D,iBAAK,OAAO;AAAA,cACV,6DAA6D,KAAK;AAAA,gBAChE;AAAA,cACF,CAAC;AAAA,YACH;AACA,kBAAM;AAAA,UACR,CAAC;AACD,wBAAc,QAAQ,MAAM;AAAA,QAC9B,SAAS,OAAO;AACd,eAAK,OAAO,MAAM,8BAA8B,KAAK;AACrD,eAAK,OAAO,MAAM,GAAG,QAAQ,OAAO,SAAS,CAAC,EAAE;AAChD,wBAAc,QAAQ,MAAM;AAAA,QAC9B;AAAA,MACF;AAAA,IACF;AAAA,EAjGA;AAAA,EAiCA;AAAA,EAwCA;AAyBF;",
  "names": []
}
